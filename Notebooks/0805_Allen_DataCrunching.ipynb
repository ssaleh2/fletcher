{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: /home/ubuntu/anaconda2/lib/python2.7/site-packages/pandas/hashtable.so: failed to map segment from shared object: Cannot allocate memory not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fcb495f0e155>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcreate_engine\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mURL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m                       \u001b[1;34m\"pandas from the source directory, you may need to run \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                       \u001b[1;34m\"'python setup.py build_ext --inplace' to build the C \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m                       \"extensions first.\".format(module))\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: C extension: /home/ubuntu/anaconda2/lib/python2.7/site-packages/pandas/hashtable.so: failed to map segment from shared object: Cannot allocate memory not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext --inplace' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import text\n",
    "import settings\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\"\n",
    "    Performs database connection using database settings from settings.py.\n",
    "    Returns sqlalchemy engine instance\n",
    "    \"\"\"\n",
    "    return create_engine(URL(**settings.DATABASE))\n",
    "\n",
    "db = db_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build new SQL tables\n",
    "\n",
    "#get list of all tables in db\n",
    "from sqlalchemy import MetaData\n",
    "start_time = time.time()\n",
    "m = MetaData(bind=db)\n",
    "m.reflect()\n",
    "tables = list(m.tables.keys())\n",
    "time.time()-start_time\n",
    "\n",
    "# from list of tables, get all w_chartevent tables\n",
    "import re\n",
    "tab_re = re.compile(r'^(w_chartevents_.*)')\n",
    "chart_tables = []\n",
    "for table in tables:\n",
    "    name=re.findall(tab_re, table)\n",
    "    if name:\n",
    "        chart_tables.append(name[0])\n",
    "w_chart_tables = sorted(chart_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine pickles (w_chart_tabes = list of tables you want to join and work on )\n",
    "df = combine_pickles(w_chart_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get what items to cut\n",
    "keep_items = pd.read_csv('../TextFiles/Keep_items.txt', names = ['itemsid'])\n",
    "keep_list = list(keep_items.itemsid.values)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#drop bad items\n",
    "count = 0\n",
    "total = len(keep_list)\n",
    "keep_index = df[df.itemid==keep_list[0]].index\n",
    "for itemid in keep_list[1:]:\n",
    "    keep_index = keep_index.append(df[df.itemid==itemid].index)\n",
    "    count +=1\n",
    "    print \"\\r\",float(count)/total * 100, \"% done\",\n",
    "print 'final step...'\n",
    "drop_index = df.index.difference(keep_index)\n",
    "df.drop(drop_index, axis = 0, inplace=True)\n",
    "print 'done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#map items\n",
    "def items_to_item(df):\n",
    "    '''\n",
    "    FUNCTION: map old item IDs to new item ID\n",
    "    INPUT = data frame\n",
    "    OUTPUT = data frame with new item IDs in place\n",
    "    '''\n",
    "    from collections import defaultdict\n",
    "    import json\n",
    "    items_dict = defaultdict(list)\n",
    "\n",
    "    with open(\"../JSONs/items_to_item.json\", 'r') as f:\n",
    "        items_dict = json.load(f)\n",
    "    items_dict = {int(k):int(v) for k,v in items_dict.items()}\n",
    "    df['itemid'].replace(items_dict, inplace = True)\n",
    "    return df\n",
    "\n",
    "df = items_to_item(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build discharge time - icustayid dictionary\n",
    "#get icustay table\n",
    "que = '''select * from icustays'''\n",
    "icustays_df = pd.read_sql(que, db)\n",
    "\n",
    "icustays_df = icustays_df[['icustay_id', 'intime','outtime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distime_dict = {}\n",
    "intime_dict = {}\n",
    "for icustay_id in df.icustay_id.unique():\n",
    "    distime_dict[icustay_id] = icustays_df[icustays_df.icustay_id == icustay_id].outtime.values[0]\n",
    "    intime_dict[icustay_id] = icustays_df[icustays_df.icustay_id == icustay_id].intime.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-4b885074ea40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#flatten chart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflatten_wchart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dt_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#flatten chart\n",
    "a = flatten_wchart(df, dt_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rename columns\n",
    "#get itemid\n",
    "d_items_df = pd.read_pickle('../Pickles/d_items.p')\n",
    "#need to create dictionary with only needed itemids\n",
    "\n",
    "df_items = df.columns.drop('icustay_id')\n",
    "itn_dict = {}\n",
    "for item in df_items:\n",
    "    itn_dict[item] = d_items_df[d_items_df.itemid == item].label.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.rename(columns = itn_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#store flattened df to pickle files\n",
    "#length = len(a)\n",
    "for i in range(4):\n",
    "    start_time = time.time()\n",
    "    filename = '../Pickles/flat_chart_' + str(i) + '.p'\n",
    "    end = (i+1)*length/4\n",
    "    start = (i)*length/4\n",
    "    a[start:end].to_pickle(filename)\n",
    "    print '\\r', time.time()-start_time, filename, 'done',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Pickles/flat_chart_0.p\n",
      "../Pickles/flat_chart_2.p\n",
      "../Pickles/flat_chart_1.p\n",
      "../Pickles/flat_chart_3.p\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#reading from pickle files\n",
    "import glob\n",
    "files = glob.glob('../Pickles/flat_*')\n",
    "dflist = []\n",
    "for filename in files:\n",
    "    print '\\r', filename\n",
    "    dflist.append(pd.read_pickle(filename))\n",
    "\n",
    "df = pd.concat(dflist)\n",
    "dflist = None\n",
    "print \"Done!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for each itemid, look at how many empty cells there are \n",
    "cols = list(df.drop('Code Status', axis = 1).columns)\n",
    "cols.remove('icustay_id')\n",
    "null_list = []\n",
    "for col in cols:\n",
    "    null_list.append([col, pd.isnull(df[col]).sum()])\n",
    "\n",
    "null_list.sort(key = lambda x:x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build target\n",
    "patient_df = pd.read_csv('../TextFiles/FINAL_patient_list.csv', index_col = 0)\n",
    "df = df.merge(patient_df[['icustay_id', 'hospital_expire_flag']], on='icustay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [x[0] for x in null_list[:]]\n",
    "features.append('icustay_id')\n",
    "features.append('hospital_expire_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove('icustay_id')\n",
    "cols.remove('hospital_expire_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1501.37234902\n"
     ]
    }
   ],
   "source": [
    "#get data from 24 hours after admit\n",
    "start_time = time.time()\n",
    "model_df = df.apply(post_admit_row, cols = cols, axis=1)\n",
    "print time.time()-start_time\n",
    "\n",
    "model_df.to_pickle('0810_24hour_post_admit_allfeat.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get time series data from vital, 24 hours after admit\n",
    "\n",
    "vit_cols = ['Arterial BP [Systolic]',\n",
    " 'Heart Rate',\n",
    " 'Arterial BP [Diastolic]',\n",
    " 'Resp Rate (Total)',\n",
    " 'SpO2',\n",
    " 'Temperature Fahrenheit',\n",
    " 'icustay_id']\n",
    "\n",
    "vit_df = df[vit_cols]\n",
    "cols = vit_cols[:]\n",
    "cols.remove('icustay_id')\n",
    "\n",
    "start_time = time.time()\n",
    "vit_model_df = vit_df.apply(post_admit_row_trans, cols = cols, axis=1)\n",
    "print time.time()-start_time\n",
    "\n",
    "vit_model_df.to_pickle('0811_trans_vit_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507.77364993\n"
     ]
    }
   ],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove('icustay_id')\n",
    "cols.remove('hospital_expire_flag')\n",
    "\n",
    "#get data from 24 hours from death\n",
    "start_time = time.time()\n",
    "pre_death_df = df.apply(before_death_row, cols = cols, axis=1)\n",
    "print time.time()-start_time\n",
    "pre_death_df.to_pickle('0811_PD_model_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194.6721549\n"
     ]
    }
   ],
   "source": [
    "#get vital transforms for 24 hours from death\n",
    "vit_df = df[vit_cols]\n",
    "cols = vit_cols[:]\n",
    "cols.remove('icustay_id')\n",
    "\n",
    "start_time = time.time()\n",
    "pd_vtrans_df = vit_df.apply(before_death_row_trans, cols = cols, axis=1)\n",
    "print time.time()-start_time\n",
    "\n",
    "pd_vtrans_df.to_pickle('0811_PD_trans_model_df.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_pickles(tables):\n",
    "    start_time = time.time()\n",
    "    dflist = []\n",
    "    count = 1\n",
    "    for table in tables:\n",
    "        print \"\\r\",\"working on %s, %d/%d tables\" % (table, count, len(tables)),\n",
    "        filename = \"../Pickles/\" + table + '.p'\n",
    "        dflist.append(pickle.load(open(filename, 'rb')))\n",
    "        count +=1\n",
    "    df = pd.concat(dflist)\n",
    "    dflist=None\n",
    "    print time.time()-start_time\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten_wchart(df, dt_dict):\n",
    "    '''\n",
    "    after getting all w_charts into pandas:\n",
    "    build features!\n",
    "    '''\n",
    "    print 'starting up...', \n",
    "    start_time = time.time()\n",
    "    count = 0\n",
    "    row_list = []\n",
    "    keep_columns = ['charttime', 'value', 'valuenum', 'valueuom', 'icustay_id', 'itemid']\n",
    "    df = df[keep_columns]\n",
    "    \n",
    "    grouped_chart = df.groupby('icustay_id')\n",
    "    total = len(grouped_chart.groups.keys())\n",
    "    \n",
    "    for patient in grouped_chart.groups.keys():\n",
    "        row_dict={}\n",
    "        pat_df = grouped_chart.get_group(patient)\n",
    "        group_item = pat_df.groupby('itemid')\n",
    "        row_dict['icustay_id'] = patient\n",
    "        distime = dt_dict[patient]\n",
    "        for itemid in group_item.groups.keys():\n",
    "            item_df = group_item.get_group(itemid)\n",
    "            item_df = item_df[(distime - item_df.charttime) > pd.Timedelta('1 day')]\n",
    "            if len(item_df) < 1:\n",
    "                row_dict[itemid] = np.nan\n",
    "            else:\n",
    "                row_dict[itemid] = item_df.sort_values(by='charttime', ascending=False).values.tolist()\n",
    "        row_list.append(row_dict)\n",
    "        count += 1 #print progress\n",
    "        perc = float(count)/total * 100\n",
    "        print '\\r', perc, \n",
    "        \n",
    "    features_df = pd.DataFrame(row_list)\n",
    "    row_list = None\n",
    "    row_dict = None\n",
    "    grouped_chart = None\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wchart_data(tables, db):\n",
    "    '''Gets all tables in list from sql server, loads them into memory and stores as .pickle'''\n",
    "    for table in tables:\n",
    "        filename = table + '.p'\n",
    "        start_time = time.time()\n",
    "        command =  '''select * from %s''' % table\n",
    "        sql = text(command)\n",
    "        temp_df = pd.read_sql(sql, db)\n",
    "        print(table)\n",
    "        print(time.time()-start_time)\n",
    "        print \"Writing %s to pickle... %s\" % (table, filename)\n",
    "        filename = table + '.p'\n",
    "        start_time = time.time()\n",
    "        pickle.dump(temp_df, open(filename , \"wb\" ) )\n",
    "        print \"Done writing pickle\", time.time()-start_time\n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_recent(entries):\n",
    "    '''Sort by time, return most recent data '''\n",
    "    try:\n",
    "        sorted_entries = sorted(entries, key = lambda x: x[0])\n",
    "        return sorted_entries[-1][2]\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "def before_death_row(df_row, cols, hours=24):\n",
    "    row = {}\n",
    "    admit_lag = str(hours) + \" Hours\"\n",
    "    row['hospital_expire_flag'] = df_row.loc['hospital_expire_flag']\n",
    "    row['icustay_id'] = df_row.loc['icustay_id']\n",
    "    distime = pd.Timestamp(distime_dict[row['icustay_id']])\n",
    "    #print intime\n",
    "    for col in cols:\n",
    "        try:\n",
    "            data = [x[2] for x in df_row.loc[col] if distime - x[0] > pd.Timedelta(admit_lag)]\n",
    "            if len(data) < 1:\n",
    "                data = np.nan\n",
    "            row[col] = data[0]\n",
    "        except:\n",
    "            row[col] = np.nan\n",
    "    result_df = pd.Series(row)\n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def before_death_row_trans(df_row, cols, hours=24):\n",
    "    row = {}\n",
    "    admit_lag = str(hours) + \" Hours\"\n",
    "    row['icustay_id'] = df_row.icustay_id\n",
    "    distime = pd.Timestamp(distime_dict[row['icustay_id']])\n",
    "    #print intime\n",
    "    for col in cols:\n",
    "        median = col + '_median'\n",
    "        std = col + '_std'\n",
    "        rng = col + '_range'\n",
    "        try:\n",
    "            data = [x[2] for x in df_row.loc[col] if (distime - x[0] > pd.Timedelta(admit_lag)) & (distime - x[0] < 2*pd.Timedelta(admit_lag))]\n",
    "            if len(data) < 1:\n",
    "                data = np.nan\n",
    "            row[col] = data[0]\n",
    "            row[median] = np.median(data)\n",
    "            if len(data) > 2:\n",
    "                row[std] = np.std(data)\n",
    "                row[rng] = np.ptp(data)\n",
    "        except:\n",
    "            row[col] = np.nan\n",
    "    result_df = pd.Series(row)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_admit_row(df_row, cols, hours=24):\n",
    "    row = {}\n",
    "    admit_lag = str(hours) + \" Hours\"\n",
    "    row['hospital_expire_flag'] = df_row.loc['hospital_expire_flag']\n",
    "    row['icustay_id'] = df_row.loc['icustay_id']\n",
    "    intime = intime_dict[row['icustay_id']]\n",
    "    #print intime\n",
    "    for col in cols:\n",
    "        try:\n",
    "            data = [x[2] for x in df_row.loc[col] if x[0] - intime < pd.Timedelta(admit_lag)]\n",
    "            if len(data) < 1:\n",
    "                data = np.nan\n",
    "            row[col] = data[0]\n",
    "        except:\n",
    "            row[col] = np.nan\n",
    "    result_df = pd.Series(row)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_admit_row_trans(df_row, cols, hours=24):\n",
    "    row = {}\n",
    "    admit_lag = str(hours) + \" Hours\"\n",
    "    row['icustay_id'] = df_row.icustay_id\n",
    "    intime = intime_dict[row['icustay_id']]\n",
    "    #print intime\n",
    "    for col in cols:\n",
    "        median = col + '_median'\n",
    "        std = col + '_std'\n",
    "        rng = col + '_range'\n",
    "        try:\n",
    "            data = [x[2] for x in df_row.loc[col] if x[0] - intime < pd.Timedelta(admit_lag)]\n",
    "            if len(data) < 1:\n",
    "                data = np.nan\n",
    "            row[col] = data[0]\n",
    "            row[median] = np.median(data)\n",
    "            if len(data) > 2:\n",
    "                row[std] = np.std(data)\n",
    "                row[rng] = np.ptp(data)\n",
    "        except:\n",
    "            row[col] = np.nan\n",
    "    result_df = pd.Series(row)\n",
    "    return result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
