{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import text\n",
    "import settings\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\"\n",
    "    Performs database connection using database settings from settings.py.\n",
    "    Returns sqlalchemy engine instance\n",
    "    \"\"\"\n",
    "    return create_engine(URL(**settings.DATABASE))\n",
    "\n",
    "db = db_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command =  '''select icustay_id, first_careunit from icustays;'''\n",
    "services = pd.read_sql(command, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "services.to_pickle('../Pickles/services.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build new SQL tables\n",
    "\n",
    "#get list of all tables in db\n",
    "from sqlalchemy import MetaData\n",
    "start_time = time.time()\n",
    "m = MetaData(bind=db)\n",
    "m.reflect()\n",
    "tables = list(m.tables.keys())\n",
    "time.time()-start_time\n",
    "\n",
    "# from list of tables, get all w_chartevent tables\n",
    "import re\n",
    "tab_re = re.compile(r'^(w_chartevents_.*)')\n",
    "chart_tables = []\n",
    "for table in tables:\n",
    "    name=re.findall(tab_re, table)\n",
    "    if name:\n",
    "        chart_tables.append(name[0])\n",
    "w_chart_tables = sorted(chart_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your w_charts to work from now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'w_chartevents_1', u'w_chartevents_10', u'w_chartevents_11', u'w_chartevents_12', u'w_chartevents_13', u'w_chartevents_14', u'w_chartevents_2', u'w_chartevents_3', u'w_chartevents_4', u'w_chartevents_5', u'w_chartevents_6', u'w_chartevents_7', u'w_chartevents_8', u'w_chartevents_9']\n"
     ]
    }
   ],
   "source": [
    "## START TIME!!!!!!!\n",
    "start_time = time.time()\n",
    "#pick w_charts here\n",
    "print w_chart_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combine pickles (w_chart_tabes = list of tables you want to join and work on )\n",
    "df = combine_pickles(w_chart_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get what items to cut\n",
    "keep_items = pd.read_csv('../TextFiles/Keep_items.txt', names = ['itemsid'])\n",
    "len(keep_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_list = list(keep_items.itemsid.values)\n",
    "len(keep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Drop BAD Items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop bad items\n",
    "count = 0\n",
    "total = len(df)\n",
    "df = df[df.itemid.isin(keep_list)]\n",
    "print 'done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map old items to new items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def items_to_item(df):\n",
    "    '''\n",
    "    FUNCTION: map old item IDs to new item ID\n",
    "    INPUT = data frame\n",
    "    OUTPUT = data frame with new item IDs in place\n",
    "    '''\n",
    "    from collections import defaultdict\n",
    "    import json\n",
    "    items_dict = defaultdict(list)\n",
    "\n",
    "    with open(\"../JSONs/items_to_item.json\", 'r') as f:\n",
    "        items_dict = json.load(f)\n",
    "    items_dict = {int(k):int(v) for k,v in items_dict.items()}\n",
    "    df['itemid'].replace(items_dict, inplace = True)\n",
    "    return df\n",
    "\n",
    "df = items_to_item(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df.itemid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dictionary of OUT times from ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "que = '''select * from icustay_detail;'''\n",
    "icustays_df = pd.read_sql(que, db)\n",
    "icustays_df = icustays_df[['icustay_id', 'outtime']]\n",
    "print icustays_df.head()\n",
    "icustays_df = icustays_df[icustays_df.icustay_id.isin(df.icustay_id.unique())]\n",
    "print len(icustays_df.icustay_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_dict = icustays_df.set_index('icustay_id')['outtime'].to_dict()\n",
    "print \"DONE!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grouped_chart = df.groupby('icustay_id')\n",
    "# pat_df = grouped_chart.get_group(grouped_chart.groups.keys()[1])\n",
    "# group_item = pat_df.groupby('itemid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group_item.get_group(group_item.groups.keys()[1])[['charttime','value','valuenum','valueuom']].values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten_wchart(df, dt_dict, hours_before = 24):\n",
    "    '''\n",
    "    after getting all w_charts into pandas:\n",
    "    build features!\n",
    "    '''\n",
    "    print 'starting up...', \n",
    "    #start_time = time.time()\n",
    "    count = 0\n",
    "    row_list = []\n",
    "    keep_columns = ['subject_id', 'charttime', 'value', 'valuenum', 'valueuom', 'icustay_id', 'itemid']\n",
    "    df = df[keep_columns]\n",
    "    \n",
    "    grouped_chart = df.groupby('icustay_id')\n",
    "    total = len(grouped_chart.groups.keys())\n",
    "    \n",
    "    for patient in grouped_chart.groups.keys():\n",
    "        row_dict={}\n",
    "        pat_df = grouped_chart.get_group(patient)\n",
    "        group_item = pat_df.groupby('itemid')\n",
    "        row_dict['icustay_id'] = patient\n",
    "        distime = dt_dict[patient]\n",
    "        for itemid in group_item.groups.keys():\n",
    "            item_df = group_item.get_group(itemid)\n",
    "            if len(item_df) < 1:\n",
    "                row_dict[itemid] = np.nan\n",
    "            else:\n",
    "                row_dict[itemid] = item_df[(distime - item_df.charttime) > pd.Timedelta(str(hours_before) + 'hours')].sort_values(by='charttime', \n",
    "                                        ascending=False)[['charttime','value','valuenum','valueuom']].values.tolist()\n",
    "        row_list.append(row_dict)\n",
    "        count += 1 #print progress\n",
    "        perc = float(count)/total * 100\n",
    "        print '\\r{0} %done'.format(perc),\n",
    "        \n",
    "    features_df = pd.DataFrame(row_list)\n",
    "    row_list = None\n",
    "    row_dict = None\n",
    "    grouped_chart = None\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change hours before to chop out as desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hours_before = 48\n",
    "final_df = flatten_wchart(df, dt_dict, hours_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change from Item ID to Item Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../JSONs/item_to_name.json\", 'r') as f:  # Get one item ID for item groups\n",
    "    itemTname_key_dict = json.load(f)\n",
    "itemTname_key_dict = {int(k):str(v) for k,v in itemTname_key_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.rename(columns = itemTname_key_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add target and demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patient_df = pd.read_csv('../TextFiles/FINAL_patient_list.csv')\n",
    "patient_df = patient_df.drop(['Unnamed: 0', 'index', 'hadm_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_patient_df = patient_df.merge(final_df, on = 'icustay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_patient_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write FINAL df to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = '../Pickles/%d_final_patient_df.p' %(hours_before)\n",
    "final_patient_df.to_pickle(name)\n",
    "print \"TIME ELAPSED: %f\" %d(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store final df to several pickle files as backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-6-6c5aa4644b97>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-6c5aa4644b97>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    filename = '../Pickles/backup/' + str(hours_before) + \"short_final_df_' + str(i) + '.p'\u001b[0m\n\u001b[0m                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "def save_file_pickle(df, num_split, hours_before):\n",
    "    length = len(df)\n",
    "    for i in range(num_split):\n",
    "        start_time = time.time()\n",
    "        filename = '../Pickles/backup/' + str(hours_before) + \"short_final_df_' + str(i) + '.p'\n",
    "        end = (i+1)*length/num_split\n",
    "        start = (i)*length/num_split\n",
    "        df[start:end].to_pickle(filename)\n",
    "        print '\\r', time.time()-start_time, filename, 'done',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.5171160698 ../Pickles/backup/short_final_df_3.p done\n"
     ]
    }
   ],
   "source": [
    "save_file_pickle(final_patient_df, 4, hours_before)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### After flattening, split and send to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_pickles(tables):\n",
    "    dflist = []\n",
    "    count = 1\n",
    "    for table in tables:\n",
    "        print \"\\r\",\"working on %s, %d/%d tables\" % (table, count, len(tables)),\n",
    "        filename = \"../Pickles/\" + table + '.p'\n",
    "        dflist.append(pickle.load(open(filename, 'rb')))\n",
    "        count +=1\n",
    "    df = pd.concat(dflist)\n",
    "    dflist=None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wchart_data(tables, db):\n",
    "    '''Gets all tables in list from sql server, loads them into memory and stores as .pickle'''\n",
    "    for table in tables:\n",
    "        filename = table + '.p'\n",
    "        start_time = time.time()\n",
    "        command =  '''select * from %s''' % table\n",
    "        sql = text(command)\n",
    "        temp_df = pd.read_sql(sql, db)\n",
    "        print(table)\n",
    "        print(time.time()-start_time)\n",
    "        print \"Writing %s to pickle... %s\" % (table, filename)\n",
    "        filename = table + '.p'\n",
    "        start_time = time.time()\n",
    "        pickle.dump(temp_df, open(filename , \"wb\" ) )\n",
    "        del temp_df\n",
    "        print \"Done writing pickle\", time.time()-start_time\n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_recent(entries):\n",
    "    '''Sort by time, return most recent data '''\n",
    "    sorted_entries = sorted(entries, key = lambda x: x[0])\n",
    "    return sorted_entries[-1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
