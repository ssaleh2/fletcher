{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import text\n",
    "import settings\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "def db_connect():\n",
    "    \"\"\"\n",
    "    Performs database connection using database settings from settings.py.\n",
    "    Returns sqlalchemy engine instance\n",
    "    \"\"\"\n",
    "    return create_engine(URL(**settings.DATABASE))\n",
    "\n",
    "db = db_connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "command =  '''select icustay_id, first_careunit from icustays;'''\n",
    "services = pd.read_sql(command, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "services.to_pickle('../Pickles/services.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-345b1e6011a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfinal_patient_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpatient_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'icustay_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'final_df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_patient_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build new SQL tables\n",
    "\n",
    "#get list of all tables in db\n",
    "from sqlalchemy import MetaData\n",
    "start_time = time.time()\n",
    "m = MetaData(bind=db)\n",
    "m.reflect()\n",
    "tables = list(m.tables.keys())\n",
    "time.time()-start_time\n",
    "\n",
    "# from list of tables, get all w_chartevent tables\n",
    "import re\n",
    "tab_re = re.compile(r'^(w_chartevents_.*)')\n",
    "chart_tables = []\n",
    "for table in tables:\n",
    "    name=re.findall(tab_re, table)\n",
    "    if name:\n",
    "        chart_tables.append(name[0])\n",
    "w_chart_tables = sorted(chart_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose your w_charts to work from now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'w_chartevents_1', u'w_chartevents_10', u'w_chartevents_11', u'w_chartevents_12', u'w_chartevents_13', u'w_chartevents_14', u'w_chartevents_2', u'w_chartevents_3', u'w_chartevents_4', u'w_chartevents_5', u'w_chartevents_6', u'w_chartevents_7', u'w_chartevents_8', u'w_chartevents_9']\n"
     ]
    }
   ],
   "source": [
    "## START TIME!!!!!!!\n",
    "start_time = time.time()\n",
    "#pick w_charts here\n",
    "print w_chart_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on w_chartevents_9, 14/14 tables"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-b03472f6dd04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#combine pickles (w_chart_tabes = list of tables you want to join and work on )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombine_pickles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_chart_tables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-49579b3b8710>\u001b[0m in \u001b[0;36mcombine_pickles\u001b[1;34m(tables)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mdflist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdflist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mdflist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cynthia/local/anaconda2/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    844\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m                        copy=copy)\n\u001b[1;32m--> 846\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cynthia/local/anaconda2/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                                   \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1038\u001b[1;33m                                                   copy=self.copy)\n\u001b[0m\u001b[0;32m   1039\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cynthia/local/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   4543\u001b[0m                                                 copy=copy),\n\u001b[0;32m   4544\u001b[0m                          placement=placement)\n\u001b[1;32m-> 4545\u001b[1;33m               for placement, join_units in concat_plan]\n\u001b[0m\u001b[0;32m   4546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4547\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cynthia/local/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m   4648\u001b[0m             \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4649\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4650\u001b[1;33m         \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4652\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cynthia/local/anaconda2/lib/python2.7/site-packages/pandas/types/concat.pyc\u001b[0m in \u001b[0;36m_concat_compat\u001b[1;34m(to_concat, axis)\u001b[0m\n\u001b[0;32m    142\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#combine pickles (w_chart_tabes = list of tables you want to join and work on )\n",
    "df = combine_pickles(w_chart_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53725693\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>itemid</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>value</th>\n",
       "      <th>valuenum</th>\n",
       "      <th>valueuom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105444129.0</td>\n",
       "      <td>14021.0</td>\n",
       "      <td>275832.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2193-09-21 20:00:00</td>\n",
       "      <td>2193-09-21 20:32:00</td>\n",
       "      <td>Full Code</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>105444134.0</td>\n",
       "      <td>14021.0</td>\n",
       "      <td>275832.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2193-09-21 20:00:00</td>\n",
       "      <td>2193-09-21 20:32:00</td>\n",
       "      <td>3 To speech</td>\n",
       "      <td>3.0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>105444277.0</td>\n",
       "      <td>14021.0</td>\n",
       "      <td>275832.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2193-09-21 23:00:00</td>\n",
       "      <td>2193-09-21 23:34:00</td>\n",
       "      <td>Full Code</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>105444322.0</td>\n",
       "      <td>14021.0</td>\n",
       "      <td>275832.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2193-09-22 00:00:00</td>\n",
       "      <td>2193-09-21 23:56:00</td>\n",
       "      <td>Full Code</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105444637.0</td>\n",
       "      <td>14021.0</td>\n",
       "      <td>275832.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>2193-09-22 05:00:00</td>\n",
       "      <td>2193-09-22 05:17:00</td>\n",
       "      <td>Full Code</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        row_id  subject_id  icustay_id  itemid           charttime  \\\n",
       "0  105444129.0     14021.0    275832.0   128.0 2193-09-21 20:00:00   \n",
       "1  105444134.0     14021.0    275832.0   184.0 2193-09-21 20:00:00   \n",
       "2  105444277.0     14021.0    275832.0   128.0 2193-09-21 23:00:00   \n",
       "3  105444322.0     14021.0    275832.0   128.0 2193-09-22 00:00:00   \n",
       "4  105444637.0     14021.0    275832.0   128.0 2193-09-22 05:00:00   \n",
       "\n",
       "            storetime        value  valuenum valueuom  \n",
       "0 2193-09-21 20:32:00    Full Code       NaN           \n",
       "1 2193-09-21 20:32:00  3 To speech       3.0           \n",
       "2 2193-09-21 23:34:00    Full Code       NaN           \n",
       "3 2193-09-21 23:56:00    Full Code       NaN           \n",
       "4 2193-09-22 05:17:00    Full Code       NaN           "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get what items to cut\n",
    "keep_items = pd.read_csv('../TextFiles/Keep_items.txt', names = ['itemsid'])\n",
    "len(keep_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keep_list = list(keep_items.itemsid.values)\n",
    "len(keep_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Drop BAD Items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop bad items\n",
    "count = 0\n",
    "total = len(df)\n",
    "df = df[df.itemid.isin(keep_list)]\n",
    "print 'done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map old items to new items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def items_to_item(df):\n",
    "    '''\n",
    "    FUNCTION: map old item IDs to new item ID\n",
    "    INPUT = data frame\n",
    "    OUTPUT = data frame with new item IDs in place\n",
    "    '''\n",
    "    from collections import defaultdict\n",
    "    import json\n",
    "    items_dict = defaultdict(list)\n",
    "\n",
    "    with open(\"../JSONs/items_to_item.json\", 'r') as f:\n",
    "        items_dict = json.load(f)\n",
    "    items_dict = {int(k):int(v) for k,v in items_dict.items()}\n",
    "    df['itemid'].replace(items_dict, inplace = True)\n",
    "    return df\n",
    "\n",
    "df = items_to_item(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df.itemid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dictionary of OUT times from ICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "que = '''select * from icustay_detail;'''\n",
    "icustays_df = pd.read_sql(que, db)\n",
    "icustays_df = icustays_df[['icustay_id', 'outtime']]\n",
    "print icustays_df.head()\n",
    "icustays_df = icustays_df[icustays_df.icustay_id.isin(df.icustay_id.unique())]\n",
    "print len(icustays_df.icustay_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_dict = icustays_df.set_index('icustay_id')['outtime'].to_dict()\n",
    "print \"DONE!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# grouped_chart = df.groupby('icustay_id')\n",
    "# pat_df = grouped_chart.get_group(grouped_chart.groups.keys()[1])\n",
    "# group_item = pat_df.groupby('itemid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# group_item.get_group(group_item.groups.keys()[1])[['charttime','value','valuenum','valueuom']].values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flatten_wchart(df, dt_dict, hours_before = 24):\n",
    "    '''\n",
    "    after getting all w_charts into pandas:\n",
    "    build features!\n",
    "    '''\n",
    "    print 'starting up...', \n",
    "    #start_time = time.time()\n",
    "    count = 0\n",
    "    row_list = []\n",
    "    keep_columns = ['subject_id', 'charttime', 'value', 'valuenum', 'valueuom', 'icustay_id', 'itemid']\n",
    "    df = df[keep_columns]\n",
    "    \n",
    "    grouped_chart = df.groupby('icustay_id')\n",
    "    total = len(grouped_chart.groups.keys())\n",
    "    \n",
    "    for patient in grouped_chart.groups.keys():\n",
    "        row_dict={}\n",
    "        pat_df = grouped_chart.get_group(patient)\n",
    "        group_item = pat_df.groupby('itemid')\n",
    "        row_dict['icustay_id'] = patient\n",
    "        distime = dt_dict[patient]\n",
    "        for itemid in group_item.groups.keys():\n",
    "            item_df = group_item.get_group(itemid)\n",
    "            if len(item_df) < 1:\n",
    "                row_dict[itemid] = np.nan\n",
    "            else:\n",
    "                row_dict[itemid] = item_df[(distime - item_df.charttime) > pd.Timedelta(str(hours_before) + 'hours')].sort_values(by='charttime', \n",
    "                                        ascending=False)[['charttime','value','valuenum','valueuom']].values.tolist()\n",
    "        row_list.append(row_dict)\n",
    "        count += 1 #print progress\n",
    "        perc = float(count)/total * 100\n",
    "        print '\\r{0} %done'.format(perc),\n",
    "        \n",
    "    features_df = pd.DataFrame(row_list)\n",
    "    row_list = None\n",
    "    row_dict = None\n",
    "    grouped_chart = None\n",
    "    \n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change hours before to chop out as desired!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hours_before = 48\n",
    "final_df = flatten_wchart(df, dt_dict, hours_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change from Item ID to Item Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../JSONs/item_to_name.json\", 'r') as f:  # Get one item ID for item groups\n",
    "    itemTname_key_dict = json.load(f)\n",
    "itemTname_key_dict = {int(k):str(v) for k,v in itemTname_key_dict.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.rename(columns = itemTname_key_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add target and demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patient_df = pd.read_csv('../TextFiles/FINAL_patient_list.csv')\n",
    "patient_df = patient_df.drop(['Unnamed: 0', 'index', 'hadm_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_patient_df = patient_df.merge(final_df, on = 'icustay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_patient_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write FINAL df to pickles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = '../Pickles/%d_final_patient_df.p' %(hours_before)\n",
    "final_patient_df.to_pickle(name)\n",
    "print \"TIME ELAPSED: %f\" %d(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### store final df to several pickle files as backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_file_pickle(df, num_split, hours_before):\n",
    "    length = len(df)\n",
    "    for i in range(num_split):\n",
    "        start_time = time.time()\n",
    "        filename = '../Pickles/backup/' + str(hours_before) + \"short_final_df_' + str(i) + '.p'\n",
    "        end = (i+1)*length/num_split\n",
    "        start = (i)*length/num_split\n",
    "        df[start:end].to_pickle(filename)\n",
    "        print '\\r', time.time()-start_time, filename, 'done',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71.5171160698 ../Pickles/backup/short_final_df_3.p done\n"
     ]
    }
   ],
   "source": [
    "save_file_pickle(final_patient_df, 4, hours_before)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### After flattening, split and send to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_pickles(tables):\n",
    "    dflist = []\n",
    "    count = 1\n",
    "    for table in tables:\n",
    "        print \"\\r\",\"working on %s, %d/%d tables\" % (table, count, len(tables)),\n",
    "        filename = \"../Pickles/\" + table + '.p'\n",
    "        dflist.append(pickle.load(open(filename, 'rb')))\n",
    "        count +=1\n",
    "    df = pd.concat(dflist)\n",
    "    dflist=None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wchart_data(tables, db):\n",
    "    '''Gets all tables in list from sql server, loads them into memory and stores as .pickle'''\n",
    "    for table in tables:\n",
    "        filename = table + '.p'\n",
    "        start_time = time.time()\n",
    "        command =  '''select * from %s''' % table\n",
    "        sql = text(command)\n",
    "        temp_df = pd.read_sql(sql, db)\n",
    "        print(table)\n",
    "        print(time.time()-start_time)\n",
    "        print \"Writing %s to pickle... %s\" % (table, filename)\n",
    "        filename = table + '.p'\n",
    "        start_time = time.time()\n",
    "        pickle.dump(temp_df, open(filename , \"wb\" ) )\n",
    "        print \"Done writing pickle\", time.time()-start_time\n",
    "    return 'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_most_recent(entries):\n",
    "    '''Sort by time, return most recent data '''\n",
    "    sorted_entries = sorted(entries, key = lambda x: x[0])\n",
    "    return sorted_entries[-1][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
